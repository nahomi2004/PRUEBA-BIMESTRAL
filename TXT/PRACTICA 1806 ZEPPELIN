# CODIGO UNO

val NUM_SAMPLES = 100
val count = sc.parallelize(1 to NUM_SAMPLES). filter {_ =>
    val x = math.random
    val y = math.random
    x*x + y*y < 1
}.count()
println("Pi is roufhly %f\n", 4.0 * count / NUM_SAMPLES)

## SALIDA

(Pi is roufhly %f
,2.88)
NUM_SAMPLES: Int = 100
count: Long = 72

# CODIGO UNO PERO CHATGPT

val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>
  val x = Math.random()
  val y = Math.random()
  x*x + y*y < 1
}.count()

val piEstimate = 4.0 * count / NUM_SAMPLES
println(f"Pi is roughly $piEstimate%.2f")

## SALIDA

Pi is roughly 2.80
count: Long = 70
piEstimate: Double = 2.8

# CODIGO DOS

import org.apache.spark.sql.functions.avg
// Create a DataFrame using SparkSession
val dataDF = spark.createDataFrame(Seq(("Broke", 20), ("Denny", 31), ("Jules", 30), ("TD", 35),
("Jules", 20), ("TD", 15))).toDF("name", "age")
// Group the same names together, aggregate their age, and compute an average
val avgDF = dataDF.groupBy("name").agg(avg("age"))
// Show the results of the final execution
avgDF.show()
avgDF.explain()

## SALIDA

+-----+--------+
| name|avg(age)|
+-----+--------+
|Broke|    20.0|
|Denny|    31.0|
|Jules|    25.0|
|   TD|    25.0|
+-----+--------+

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[name#4], functions=[avg(age#5)])
   +- Exchange hashpartitioning(name#4, 200), ENSURE_REQUIREMENTS, [plan_id=42]
      +- HashAggregate(keys=[name#4], functions=[partial_avg(age#5)])
         +- LocalTableScan [name#4, age#5]


import org.apache.spark.sql.functions.avg
dataDF: org.apache.spark.sql.DataFrame = [name: string, age: int]
avgDF: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]

# CODIGO CUATRO
la salida es texto

%md
# DATA FRAME

Es bastante similar a una hoja de cálculo, lo diferente es que trabaja con Spark.
El DataFrame de Spark se puede dividir y ubicarse en miles de computadoras (la razón el tamaño).
Si se tienen muchos datos, lo mejor es crear el esquema manualmente.

# CODIGO 5 Y SEIS CON ERRORES E INCOMPLETO

val data = spark
  .read
  .option("inferSchema", "true")
  .option("header", "true")
  .option("delimiter", ";")
  .csv("file:///C:\/Users\/D E L L\/Desktop\/practica1806-csv1.csv")

import org.apache.spark.sql.types._
val myDataSquema = StructType(
    Array(
        StructField('id', IntegerType, true),
        StructField
    ))
